<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Day 54: Joint Distributions</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500;600;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="initMath();"></script>
    <link rel="stylesheet" href="../../lessons/shared-styles.css">
    <script src="../../lessons/questions-data.js"></script>
    <script src="../../lessons/shared-scripts.js"></script>
</head>

<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../../index.html" class="nav-back">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M10 12L6 8L10 4" />
                </svg>
                Back to Curriculum
            </a>
            <div class="nav-progress">
                <span id="progressText">0 / 25 complete</span>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
            </div>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" />
                </svg>
            </button>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-label">
            <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                <path d="M8 0L10 6H16L11 9.5L13 16L8 12L3 16L5 9.5L0 6H6L8 0Z" />
            </svg>
            Day 54 ¬∑ The Multi-Dimensional World
        </div>
        <h1>Joint Distributions</h1>
        <p class="hero-desc">
            Randomness rarely happens in isolation. We move beyond single variables to study how multiple random
            factors interact, overlap, and influence each other‚Äîthe foundation of multivariate statistics and machine
            learning.
        </p>
        <div class="hero-meta">
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <circle cx="12" cy="12" r="10" />
                    <path d="M12 6v6l4 2" />
                </svg>
                ~90 min read
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path
                        d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2" />
                </svg>
                25 practice problems
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <polygon points="5 3 19 12 5 21 5 3" />
                </svg>
                8 video lessons
            </div>
        </div>
    </header>

    <div class="main-layout">
        <aside class="sidebar">
            <nav class="toc">
                <div class="toc-title">On this page</div>
                <ul class="toc-list">
                    <li><a href="#why" class="toc-link">Why Joint Distributions?</a></li>
                    <li><a href="#joint" class="toc-link">1. Joint PMF & PDF</a></li>
                    <li><a href="#marginal" class="toc-link">2. Marginal Distributions</a></li>
                    <li><a href="#conditional" class="toc-link">3. Conditional Distributions</a></li>
                    <li><a href="#independence" class="toc-link">4. Independence</a></li>
                    <li><a href="#covariance" class="toc-link">5. Covariance & Correlation</a></li>
                    <li><a href="#cs-connection" class="toc-link">6. CS: Multi-Armed Bandits</a></li>
                    <li><a href="#videos" class="toc-link">7. Video Lessons</a></li>
                    <li><a href="#practice" class="toc-link">8. Practice Problems</a></li>
                </ul>
            </nav>
            <div class="stats-card">
                <div class="toc-title">Your Progress</div>
                <div class="stats-row"><span class="stats-label">Attempted</span><span class="stats-value"
                        id="statAttempted">0</span></div>
                <div class="stats-row"><span class="stats-label">Correct</span><span class="stats-value"
                        id="statCorrect">0</span></div>
                <div class="stats-row"><span class="stats-label">Accuracy</span><span class="stats-value"
                        id="statAccuracy">‚Äî</span></div>
            </div>
        </aside>

        <main class="content">
            <!-- Why This Matters -->
            <section class="section" id="why">
                <div class="section-header">
                    <div class="section-number">Section 0</div>
                    <h2 class="section-title">Why Joint Distributions Matter in CS</h2>
                </div>
                <div class="section-body">
                    <p>
                        In the real world, data points don't live in isolation. A user's <strong>click rate</strong>
                        depends on both the <strong>ad content</strong> and the <strong>time of day</strong>. A system's
                        <strong>latency</strong> depends on both <strong>CPU load</strong> and <strong>network
                            traffic</strong>.
                        Machine learning models with multiple features are fundamentally reasoning about joint
                        distributions.
                    </p>

                    <div class="rule">
                        <strong>The Big Idea:</strong> A joint distribution captures everything about how two (or more)
                        random variables behave together. From it, you can derive marginals, conditionals, correlations,
                        and independence‚Äîall the tools you need for multivariate analysis.
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Where you'll use this</h3>
                        <ul>
                            <li><strong>Machine Learning:</strong> Features are joint RVs; models learn P(Y|X‚ÇÅ, X‚ÇÇ, ...)
                            </li>
                            <li><strong>Bayesian Networks:</strong> Graphical models factor joint distributions</li>
                            <li><strong>A/B Testing:</strong> Analyzing correlated metrics (conversion vs. revenue)</li>
                            <li><strong>Recommendation Systems:</strong> User-item interaction matrices</li>
                            <li><strong>Risk Analysis:</strong> Portfolio optimization with correlated assets</li>
                        </ul>
                    </div>

                    <div class="info">
                        <strong>üé≤ Simple: The Dice Table Analogy</strong>
                        <br>Imagine rolling two dice. Each die alone has 6 outcomes. But <em>together</em>, they form a
                        6√ó6 grid of 36 joint outcomes. Some outcomes (like sum=7) are more likely than others (sum=2).
                        The joint distribution is this entire table‚Äîknowing it tells you everything about both dice
                        <em>and</em> their relationships.
                    </div>
                </div>
            </section>

            <!-- Joint PMF & PDF -->
            <section class="section" id="joint">
                <div class="section-header">
                    <div class="section-number">Section 1</div>
                    <h2 class="section-title">Joint PMF & PDF</h2>
                </div>
                <div class="section-body">
                    <p>When studying two random variables $X$ and $Y$ simultaneously, we use a <strong>Joint
                            Distribution</strong>.</p>

                    <div class="subsection">
                        <h3 class="subsection-title">Discrete Case: Joint PMF</h3>
                        <p>For discrete RVs, the <strong>Joint Probability Mass Function</strong> gives the probability
                            of each (x, y) pair:</p>
                        <div class="math-block">
                            $$ p_{X,Y}(x, y) = P(X=x \text{ and } Y=y) $$
                        </div>
                        <div class="rule">
                            <strong>Requirement:</strong> All probabilities must sum to 1:
                            $$ \sum_x \sum_y p(x, y) = 1 $$
                        </div>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Continuous Case: Joint PDF</h3>
                        <p>For continuous RVs, we use a <strong>Joint Probability Density Function</strong>:</p>
                        <div class="math-block">
                            $$ P((X,Y) \in A) = \iint_{A} f_{X,Y}(x, y) \, dx \, dy $$
                        </div>
                        <p>The probability that $(X, Y)$ falls in a region is the volume under the surface $f(x,y)$ over
                            that region.</p>
                    </div>

                    <div class="info">
                        <strong>üìä Simple: The Heatmap Analogy</strong>
                        <br>Think of a joint PDF as a heatmap or topographic map. The "hotter" (taller) a region,
                        the more likely those (x, y) values are. A mountain peak at (2, 3) means X‚âà2 and Y‚âà3 is
                        the most likely outcome. Valleys are rare events.
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Visualizing a bivariate normal distribution</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">import</span> matplotlib.pyplot <span class="code-keyword">as</span> plt
<span class="code-keyword">from</span> scipy.stats <span class="code-keyword">import</span> multivariate_normal

<span class="code-comment"># Define a bivariate normal distribution</span>
mean = [<span class="code-number">0</span>, <span class="code-number">0</span>]
cov = [[<span class="code-number">1</span>, <span class="code-number">0.5</span>],    <span class="code-comment"># Covariance matrix</span>
       [<span class="code-number">0.5</span>, <span class="code-number">1</span>]]

<span class="code-comment"># Create grid of points</span>
x = np.<span class="code-function">linspace</span>(-<span class="code-number">3</span>, <span class="code-number">3</span>, <span class="code-number">100</span>)
y = np.<span class="code-function">linspace</span>(-<span class="code-number">3</span>, <span class="code-number">3</span>, <span class="code-number">100</span>)
X, Y = np.<span class="code-function">meshgrid</span>(x, y)
pos = np.<span class="code-function">dstack</span>((X, Y))

<span class="code-comment"># Evaluate the PDF at each point</span>
rv = <span class="code-function">multivariate_normal</span>(mean, cov)
Z = rv.<span class="code-function">pdf</span>(pos)

<span class="code-comment"># Plot as heatmap</span>
plt.<span class="code-function">contourf</span>(X, Y, Z, levels=<span class="code-number">20</span>, cmap=<span class="code-string">'viridis'</span>)
plt.<span class="code-function">colorbar</span>(label=<span class="code-string">'Probability Density'</span>)
plt.<span class="code-function">xlabel</span>(<span class="code-string">'X'</span>)
plt.<span class="code-function">ylabel</span>(<span class="code-string">'Y'</span>)
plt.<span class="code-function">title</span>(<span class="code-string">'Joint PDF: Bivariate Normal (œÅ=0.5)'</span>)
plt.<span class="code-function">show</span>()</pre>
                        </div>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Joint Distribution Table (Discrete Example)</h3>
                        <p>Consider two binary RVs: $X$ = "Clicked ad" and $Y$ = "Made purchase":</p>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th></th>
                                    <th>Y=0 (No Purchase)</th>
                                    <th>Y=1 (Purchased)</th>
                                    <th>Marginal P(X)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>X=0 (No Click)</strong></td>
                                    <td>0.50</td>
                                    <td>0.05</td>
                                    <td>0.55</td>
                                </tr>
                                <tr>
                                    <td><strong>X=1 (Clicked)</strong></td>
                                    <td>0.30</td>
                                    <td>0.15</td>
                                    <td>0.45</td>
                                </tr>
                                <tr>
                                    <td><strong>Marginal P(Y)</strong></td>
                                    <td>0.80</td>
                                    <td>0.20</td>
                                    <td>1.00</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </section>

            <!-- Marginal Distributions -->
            <section class="section" id="marginal">
                <div class="section-header">
                    <div class="section-number">Section 2</div>
                    <h2 class="section-title">Marginal Distributions</h2>
                </div>
                <div class="section-body">
                    <p>The <strong>Marginal Distribution</strong> of $X$ is the distribution of $X$ alone, ignoring $Y$.
                        We find it by "summing out" (discrete) or "integrating out" (continuous) the other variable.</p>

                    <div class="math-block">
                        <strong>Discrete:</strong> $$ p_X(x) = \sum_{y} p_{X,Y}(x, y) $$
                        <br><br>
                        <strong>Continuous:</strong> $$ f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dy $$
                    </div>

                    <div class="info">
                        <strong>üë• Simple: The Shadow Analogy</strong>
                        <br>If the joint distribution is a 3D mountain, the <strong>marginals</strong> are the 2D
                        shadows
                        cast onto the X-axis wall and Y-axis wall. You lose information about how X and Y interact,
                        but you see each variable's individual shape.
                    </div>

                    <div class="rule">
                        <strong>Key Insight:</strong> The marginal distributions can be recovered from the joint,
                        but you <em>cannot</em> recover the joint from just the marginals (unless X and Y are
                        independent).
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Computing marginals from a joint probability table</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Joint probability table from our ad example</span>
joint = np.<span class="code-function">array</span>([
    [<span class="code-number">0.50</span>, <span class="code-number">0.05</span>],  <span class="code-comment"># X=0: [Y=0, Y=1]</span>
    [<span class="code-number">0.30</span>, <span class="code-number">0.15</span>]   <span class="code-comment"># X=1: [Y=0, Y=1]</span>
])

<span class="code-comment"># Marginal of X: sum over Y (columns)</span>
p_X = joint.<span class="code-function">sum</span>(axis=<span class="code-number">1</span>)
<span class="code-function">print</span>(<span class="code-string">f"P(X=0) = {p_X[0]:.2f}, P(X=1) = {p_X[1]:.2f}"</span>)
<span class="code-comment"># Output: P(X=0) = 0.55, P(X=1) = 0.45</span>

<span class="code-comment"># Marginal of Y: sum over X (rows)</span>
p_Y = joint.<span class="code-function">sum</span>(axis=<span class="code-number">0</span>)
<span class="code-function">print</span>(<span class="code-string">f"P(Y=0) = {p_Y[0]:.2f}, P(Y=1) = {p_Y[1]:.2f}"</span>)
<span class="code-comment"># Output: P(Y=0) = 0.80, P(Y=1) = 0.20</span>

<span class="code-comment"># Verify: marginals should sum to 1</span>
<span class="code-keyword">assert</span> np.<span class="code-function">isclose</span>(p_X.<span class="code-function">sum</span>(), <span class="code-number">1.0</span>)
<span class="code-keyword">assert</span> np.<span class="code-function">isclose</span>(p_Y.<span class="code-function">sum</span>(), <span class="code-number">1.0</span>)</pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Conditional Distributions -->
            <section class="section" id="conditional">
                <div class="section-header">
                    <div class="section-number">Section 3</div>
                    <h2 class="section-title">Conditional Distributions</h2>
                </div>
                <div class="section-body">
                    <p>The <strong>Conditional Distribution</strong> of $X$ given $Y=y$ tells us how $X$ behaves
                        when we know the value of $Y$. This is the heart of machine learning prediction.</p>

                    <div class="math-block">
                        <strong>Discrete:</strong> $$ p_{X|Y}(x|y) = \frac{p_{X,Y}(x, y)}{p_Y(y)} $$
                        <br><br>
                        <strong>Continuous:</strong> $$ f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)} $$
                    </div>

                    <div class="rule">
                        <strong>The Chain Rule of Probability:</strong>
                        $$ p(x, y) = p(x|y) \cdot p(y) = p(y|x) \cdot p(x) $$
                        This lets you decompose joints into conditionals and marginals‚Äîessential for Bayesian reasoning.
                    </div>

                    <div class="info">
                        <strong>üîç Simple: The Filter Analogy</strong>
                        <br>Conditioning is like applying a filter. The full distribution is everyone in a company.
                        The conditional distribution P(Salary | Department=Engineering) is everyone in a company
                        <em>filtered to just engineers</em>. The shape changes because you're looking at a subset.
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Computing conditional probabilities</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Joint probability table</span>
joint = np.<span class="code-function">array</span>([
    [<span class="code-number">0.50</span>, <span class="code-number">0.05</span>],  <span class="code-comment"># X=0: [Y=0, Y=1]</span>
    [<span class="code-number">0.30</span>, <span class="code-number">0.15</span>]   <span class="code-comment"># X=1: [Y=0, Y=1]</span>
])

<span class="code-comment"># P(Y|X=1): Given user clicked, what's purchase probability?</span>
p_X1 = joint[<span class="code-number">1</span>, :].<span class="code-function">sum</span>()  <span class="code-comment"># P(X=1) = 0.45</span>
p_Y_given_X1 = joint[<span class="code-number">1</span>, :] / p_X1
<span class="code-function">print</span>(<span class="code-string">f"P(Y=1|X=1) = {p_Y_given_X1[1]:.3f}"</span>)
<span class="code-comment"># Output: P(Y=1|X=1) = 0.333 (33% of clickers purchase)</span>

<span class="code-comment"># P(X|Y=1): Given purchase, what's click probability?</span>
p_Y1 = joint[:, <span class="code-number">1</span>].<span class="code-function">sum</span>()  <span class="code-comment"># P(Y=1) = 0.20</span>
p_X_given_Y1 = joint[:, <span class="code-number">1</span>] / p_Y1
<span class="code-function">print</span>(<span class="code-string">f"P(X=1|Y=1) = {p_X_given_Y1[1]:.3f}"</span>)
<span class="code-comment"># Output: P(X=1|Y=1) = 0.750 (75% of purchasers clicked!)</span>

<span class="code-comment"># This asymmetry is crucial for attribution modeling</span></pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Independence -->
            <section class="section" id="independence">
                <div class="section-header">
                    <div class="section-number">Section 4</div>
                    <h2 class="section-title">Independence of Random Variables</h2>
                </div>
                <div class="section-body">
                    <p>Two random variables are <strong>independent</strong> if knowing one tells you nothing about the
                        other.
                        This is the strongest form of "no relationship."</p>

                    <div class="math-block">
                        $$ X \perp Y \iff p_{X,Y}(x, y) = p_X(x) \cdot p_Y(y) \text{ for all } x, y $$
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Equivalent Conditions for Independence</h3>
                        <ul>
                            <li>$p(x, y) = p(x) \cdot p(y)$ for all x, y</li>
                            <li>$p(x|y) = p(x)$ for all x, y</li>
                            <li>$p(y|x) = p(y)$ for all x, y</li>
                            <li>$Cov(X, Y) = 0$ <em>(necessary but not sufficient!)</em></li>
                        </ul>
                    </div>

                    <div class="warning">
                        <strong>‚ö†Ô∏è Independence vs. Uncorrelated:</strong>
                        <br>Independent implies uncorrelated, but uncorrelated does NOT imply independent!
                        Example: $X \sim Uniform(-1, 1)$ and $Y = X^2$ have $Cov(X,Y) = 0$ but knowing $X$
                        completely determines $Y$.
                    </div>

                    <div class="info">
                        <strong>üéØ Simple: The Fair Coin Analogy</strong>
                        <br>Two fair coins are independent: knowing the first coin landed heads tells you
                        nothing about the second. But if someone secretly glues them together, they become
                        dependent‚Äîboth always show the same face.
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Testing for independence</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-keyword">def</span> <span class="code-function">is_independent</span>(joint, tolerance=<span class="code-number">1e-6</span>):
    <span class="code-string">"""Check if joint distribution factors into marginals."""</span>
    p_X = joint.<span class="code-function">sum</span>(axis=<span class="code-number">1</span>)  <span class="code-comment"># Marginal of X</span>
    p_Y = joint.<span class="code-function">sum</span>(axis=<span class="code-number">0</span>)  <span class="code-comment"># Marginal of Y</span>
    
    <span class="code-comment"># Compute P(X) √ó P(Y) for all pairs</span>
    product = np.<span class="code-function">outer</span>(p_X, p_Y)
    
    <span class="code-comment"># Check if joint ‚âà product</span>
    <span class="code-keyword">return</span> np.<span class="code-function">allclose</span>(joint, product, atol=tolerance)

<span class="code-comment"># Our ad click data</span>
joint_dependent = np.<span class="code-function">array</span>([[<span class="code-number">0.50</span>, <span class="code-number">0.05</span>], [<span class="code-number">0.30</span>, <span class="code-number">0.15</span>]])
<span class="code-function">print</span>(<span class="code-string">f"Ad clicks data independent? {is_independent(joint_dependent)}"</span>)
<span class="code-comment"># Output: False (clicking and purchasing are related!)</span>

<span class="code-comment"># Independent example: two fair coin flips</span>
joint_coins = np.<span class="code-function">array</span>([[<span class="code-number">0.25</span>, <span class="code-number">0.25</span>], [<span class="code-number">0.25</span>, <span class="code-number">0.25</span>]])
<span class="code-function">print</span>(<span class="code-string">f"Coin flips independent? {is_independent(joint_coins)}"</span>)
<span class="code-comment"># Output: True</span></pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Covariance & Correlation -->
            <section class="section" id="covariance">
                <div class="section-header">
                    <div class="section-number">Section 5</div>
                    <h2 class="section-title">Covariance & Correlation</h2>
                </div>
                <div class="section-body">
                    <p>Covariance and correlation quantify <em>how</em> two variables move together‚Äîdo they
                        rise and fall in sync, or in opposition?</p>

                    <div class="subsection">
                        <h3 class="subsection-title">Covariance</h3>
                        <div class="math-block">
                            $$ Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y] $$
                        </div>
                        <ul>
                            <li>$Cov(X, Y) > 0$: X and Y tend to increase together</li>
                            <li>$Cov(X, Y) < 0$: When X increases, Y tends to decrease</li>
                            <li>$Cov(X, Y) = 0$: No linear relationship (but maybe nonlinear!)</li>
                        </ul>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Correlation (Pearson)</h3>
                        <p>Correlation normalizes covariance to the range [-1, 1]:</p>
                        <div class="math-block">
                            $$ \rho_{X,Y} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y} $$
                        </div>
                        <ul>
                            <li>$\rho = 1$: Perfect positive linear relationship</li>
                            <li>$\rho = -1$: Perfect negative linear relationship</li>
                            <li>$\rho = 0$: No linear relationship</li>
                        </ul>
                    </div>

                    <div class="info">
                        <strong>üìà Simple: The Dance Partners Analogy</strong>
                        <br>High correlation = synchronized dancers moving together ($\rho \approx 1$).
                        Opposite correlation = one bows as the other stands ($\rho \approx -1$).
                        Zero correlation = dancing independently, ignoring each other ($\rho \approx 0$).
                    </div>

                    <div class="rule">
                        <strong>Important Properties:</strong>
                        <ul>
                            <li>$Cov(X, X) = Var(X)$</li>
                            <li>$Cov(aX + b, cY + d) = ac \cdot Cov(X, Y)$</li>
                            <li>$Var(X + Y) = Var(X) + Var(Y) + 2 \cdot Cov(X, Y)$</li>
                            <li>If $X \perp Y$, then $Var(X + Y) = Var(X) + Var(Y)$</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <strong>‚ö†Ô∏è Correlation ‚â† Causation!</strong>
                        <br>Ice cream sales and drowning deaths are correlated (both rise in summer),
                        but ice cream doesn't cause drowning. Always look for lurking variables!
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Computing covariance and correlation</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Sample data: study hours vs. exam score</span>
study_hours = np.<span class="code-function">array</span>([<span class="code-number">2</span>, <span class="code-number">3</span>, <span class="code-number">5</span>, <span class="code-number">7</span>, <span class="code-number">8</span>, <span class="code-number">10</span>])
exam_scores = np.<span class="code-function">array</span>([<span class="code-number">50</span>, <span class="code-number">55</span>, <span class="code-number">68</span>, <span class="code-number">76</span>, <span class="code-number">82</span>, <span class="code-number">95</span>])

<span class="code-comment"># Method 1: Manual calculation</span>
mean_x = study_hours.<span class="code-function">mean</span>()
mean_y = exam_scores.<span class="code-function">mean</span>()
cov_manual = ((study_hours - mean_x) * (exam_scores - mean_y)).<span class="code-function">mean</span>()
<span class="code-function">print</span>(<span class="code-string">f"Covariance (manual): {cov_manual:.2f}"</span>)

<span class="code-comment"># Method 2: NumPy (returns covariance matrix)</span>
cov_matrix = np.<span class="code-function">cov</span>(study_hours, exam_scores, ddof=<span class="code-number">0</span>)
<span class="code-function">print</span>(<span class="code-string">f"Covariance (numpy): {cov_matrix[0,1]:.2f}"</span>)

<span class="code-comment"># Correlation coefficient</span>
corr_matrix = np.<span class="code-function">corrcoef</span>(study_hours, exam_scores)
<span class="code-function">print</span>(<span class="code-string">f"Correlation: {corr_matrix[0,1]:.3f}"</span>)
<span class="code-comment"># Output: Correlation ‚âà 0.989 (strong positive relationship)</span></pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- CS Connection -->
            <section class="section" id="cs-connection">
                <div class="section-header">
                    <div class="section-number">Section 6</div>
                    <h2 class="section-title">CS Connection: Multi-Armed Bandits & Joint Rewards</h2>
                </div>
                <div class="section-body">
                    <p>In reinforcement learning, the <strong>multi-armed bandit</strong> problem involves
                        choosing between actions with unknown reward distributions. When rewards are correlated
                        across arms, understanding joint distributions becomes critical.</p>

                    <div class="subsection">
                        <h3 class="subsection-title">Practical Example: A/B Testing with Correlated Metrics</h3>
                        <p>When running an A/B test, you might track both <strong>click rate</strong> (X) and
                            <strong>conversion rate</strong> (Y). These metrics are jointly distributed:
                        </p>
                        <ul>
                            <li>Knowing X helps predict Y (conditional distribution)</li>
                            <li>We care about increasing <em>both</em> metrics</li>
                            <li>High correlation means improving one often improves the other</li>
                        </ul>
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Simulating correlated A/B test metrics</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> scipy.stats <span class="code-keyword">import</span> multivariate_normal

<span class="code-keyword">def</span> <span class="code-function">simulate_ab_test</span>(n_users, correlation=<span class="code-number">0.6</span>):
    <span class="code-string">"""Simulate click and conversion rates with correlation."""</span>
    <span class="code-comment"># Control group: lower mean click & conversion</span>
    mean_control = [<span class="code-number">0.05</span>, <span class="code-number">0.02</span>]  <span class="code-comment"># [click_rate, conv_rate]</span>
    
    <span class="code-comment"># Treatment group: higher means</span>
    mean_treatment = [<span class="code-number">0.08</span>, <span class="code-number">0.035</span>]
    
    <span class="code-comment"># Covariance matrix (correlated metrics)</span>
    var = <span class="code-number">0.001</span>
    cov = correlation * var
    cov_matrix = [[var, cov], [cov, var]]
    
    <span class="code-comment"># Sample from bivariate normal (truncated for rates)</span>
    control = <span class="code-function">multivariate_normal</span>(mean_control, cov_matrix).<span class="code-function">rvs</span>(n_users)
    treatment = <span class="code-function">multivariate_normal</span>(mean_treatment, cov_matrix).<span class="code-function">rvs</span>(n_users)
    
    <span class="code-keyword">return</span> control, treatment

<span class="code-comment"># Run simulation</span>
np.random.<span class="code-function">seed</span>(<span class="code-number">42</span>)
control, treatment = <span class="code-function">simulate_ab_test</span>(<span class="code-number">1000</span>)

<span class="code-function">print</span>(<span class="code-string">"Control group correlation:"</span>, np.<span class="code-function">corrcoef</span>(control.T)[<span class="code-number">0</span>,<span class="code-number">1</span>].<span class="code-function">round</span>(<span class="code-number">3</span>))
<span class="code-function">print</span>(<span class="code-string">"Avg click rate - Control:"</span>, control[:,<span class="code-number">0</span>].<span class="code-function">mean</span>().<span class="code-function">round</span>(<span class="code-number">4</span>))
<span class="code-function">print</span>(<span class="code-string">"Avg click rate - Treatment:"</span>, treatment[:,<span class="code-number">0</span>].<span class="code-function">mean</span>().<span class="code-function">round</span>(<span class="code-number">4</span>))</pre>
                        </div>
                    </div>

                    <div class="info">
                        <strong>ü§ñ Simple: The Recommendation System Analogy</strong>
                        <br>Netflix doesn't just model P(you like action movies). It models the joint
                        P(you like action movies, your friend likes action movies, it's Friday night).
                        Understanding these joint patterns powers collaborative filtering.
                    </div>
                </div>
            </section>

            <!-- Videos -->
            <section class="section" id="videos">
                <div class="section-header">
                    <div class="section-number">Section 7</div>
                    <h2 class="section-title">Video Lessons</h2>
                </div>
                <div class="section-body">
                    <div class="video-container">
                        <div class="video-player">
                            <iframe id="ytPlayer" src="" allowfullscreen></iframe>
                            <div class="video-info">
                                <div class="video-title" id="videoTitle">Select a video</div>
                                <div class="video-meta" id="videoMeta"></div>
                            </div>
                        </div>
                        <div class="video-list" id="videoList"></div>
                    </div>
                </div>
            </section>

            <!-- Practice -->
            <section class="section" id="practice">
                <div class="section-header">
                    <div class="section-number">Section 8</div>
                    <h2 class="section-title">Practice Problems</h2>
                </div>
                <div class="section-body">
                    <div class="quiz-controls">
                        <button class="btn btn-primary" id="btnCheckAll">Check All</button>
                        <button class="btn" id="btnRevealAll">Reveal All</button>
                        <button class="btn" id="btnClear">Clear Inputs</button>
                        <button class="btn btn-danger" id="btnReset">Reset Stats</button>
                    </div>
                    <div id="quizContainer"></div>
                </div>
            </section>
        </main>
    </div>

    <script src="../../lessons/questions-data.js"></script>
    <script src="../../lessons/shared-scripts.js"></script>
    <script>
        const VIDEO_GROUPS = [
            {
                title: "Joint Distributions Fundamentals",
                items: [
                    { title: "Joint Distributions Intro", channel: "Harvard Stat 110", vid: "_79v4b_5qYw" },
                    { title: "Joint PMFs Explained", channel: "Kimberly Brehm", vid: "j6u6oXW-T-8" },
                    { title: "Bivariate Distributions", channel: "3Blue1Brown", vid: "XG2XtT6MXCE" }
                ]
            },
            {
                title: "Marginals & Conditionals",
                items: [
                    { title: "Marginal Distributions", channel: "StatQuest", vid: "IT1KAlNwPa4" },
                    { title: "Conditional Probabilities", channel: "Khan Academy", vid: "ibINrxJLvlM" }
                ]
            },
            {
                title: "Covariance & Correlation",
                items: [
                    { title: "Covariance & Correlation", channel: "StatQuest", vid: "qtaqvPAeEJY" },
                    { title: "Independence of RVs", channel: "Khan Academy", vid: "vXvYid7-vYg" },
                    { title: "Multivariate Gaussian", channel: "Visually Explained", vid: "eho8xH3E6mE" }
                ]
            }
        ];

        initLesson({
            videos: VIDEO_GROUPS,
            questions: window.QUESTIONS_DATA['day54'] || [],
            storageKey: 'day54_joint_v2'
        });
    </script>
</body>

</html>